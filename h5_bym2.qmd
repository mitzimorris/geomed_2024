---
title: "Implementing the BYM2 Model in Stan"
format:
  html:
    css: theming/quarto_styles.css 
    syntax-definitions:
      - theming/stan.xml
    embed-resources: true
    toc: true
    toc-location: left
    grid:
      body-width: 1000px
execute:
  eval: false
  keep-ipynb: true
---

## Notebook Setup

### Libraries and helper functions.

**Python**
```{python}
# import all libraries used in this notebook
import os
import numpy as np
import pandas as pd
import geopandas as gpd
import libpysal as sa
import matplotlib
import splot as splt
from splot.libpysal import plot_spatial_weights 
import plotnine as p9
import arviz as az
%matplotlib inline

from cmdstanpy import CmdStanModel, cmdstan_path, cmdstan_version

# suppress plotnine warnings
import warnings
warnings.filterwarnings('ignore')

# setup plotnine look and feel
p9.theme_set(
  p9.theme_grey() + 
  p9.theme(text=p9.element_text(size=10),
        plot_title=p9.element_text(size=14),
        axis_title_x=p9.element_text(size=12),
        axis_title_y=p9.element_text(size=12),
        axis_text_x=p9.element_text(size=8),
        axis_text_y=p9.element_text(size=8)
       )
)
xlabels_90 = p9.theme(axis_text_x = p9.element_text(angle=90, hjust=1))

map_theme =  p9.theme(figure_size=(7,6),
                 axis_text_x=p9.element_blank(),
                 axis_ticks_x=p9.element_blank(),
                 axis_text_y=p9.element_blank(),
                 axis_ticks_y=p9.element_blank())
```
```{python}
# helper functions
import scipy.sparse as sp
import scipy.sparse.linalg as splinalg

def q_inv_dense(Q, A=None):
    # Compute the inverse of the sparse precision matrix Q
    Sigma = splinalg.inv(Q).todense()
    if A is None:
        return Sigma
    else:
        A = np.ones((1, Sigma.shape[0]))
        W = Sigma @ A.T
        Sigma_const = Sigma - W @ np.linalg.inv(A @ W) @ W.T
        return Sigma_const

def get_scaling_factor(nbs):
    # Compute the geometric mean of the spatial covariance matrix
    N = nbs.n

    # Create the adjacency matrix from the weights object
    adj_matrix = nbs.full()[0]
    adj_matrix = sp.csr_matrix(adj_matrix)  # Convert to sparse matrix

    # Create ICAR precision matrix (diagonal minus adjacency)
    Q = sp.diags(np.ravel(adj_matrix.sum(axis=1))) - adj_matrix
    # Add jitter to the diagonal for numerical stability
    Q_pert = Q + sp.eye(N) * np.max(Q.diagonal()) * np.sqrt(np.finfo(float).eps)

    # Compute the diagonal elements of the covariance matrix
    Q_inv = q_inv_dense(Q_pert, adj_matrix)

    # Compute the geometric mean of the variances (diagonal elements of Q_inv)
    return np.exp(np.mean(np.log(np.diag(Q_inv))))
```

**R**
```{r}
library(sf)
library(spdep) |> suppressPackageStartupMessages()
library(ggplot2)
library(tidyverse) |> suppressPackageStartupMessages()
library(cmdstanr)
library(posterior)
library(bayesplot)
library(loo)
library(parallel)
library(igraph)
library(Matrix)
cores = floor(parallel::detectCores() / 2)
```

```{r}
# helper functions

#  create edgelist for ICAR component
nbs_to_adjlist <- function(nb) {
    adj_matrix = nb2mat(nb,style="B")
    t(as_edgelist(graph_from_adjacency_matrix(adj_matrix, mode="undirected")))
}

# Compute the inverse of a sparse precision matrix
q_inv_dense <- function(Q, A = NULL) {
  Sigma <- Matrix::solve(Q)
  if (is.null(A))
    return(Sigma)
  else {
    A <- matrix(1,1, nrow(Sigma))
    W <- Sigma %*% t(A)
    Sigma_const <- Sigma - W %*% solve(A %*% W) %*% t(W)
    return(Sigma_const)
  }
}

# Compute the geometric mean of the spatial covariance matrix
get_scaling_factor = function(nbs) {
    N = length(nbs)
    # Create ICAR precision matrix  (diag - adjacency): this is singular
    adj_matrix = nb2mat(nbs,style="B")
    Q =  Diagonal(N, rowSums(adj_matrix)) - adj_matrix
    # Add a small jitter to the diagonal for numerical stability (optional but recommended)
    Q_pert = Q + Diagonal(N) * max(diag(Q)) * sqrt(.Machine$double.eps)
    # Compute the diagonal elements of the covariance matrix
    Q_inv = q_inv_dense(Q_pert, adj_matrix)
    # Compute the geometric mean of the variances, which are on the diagonal of Q_inv
    return(exp(mean(log(diag(Q_inv)))))
}
```

### Load Data

**Python**
```{python}
nyc_geodata = gpd.read_file(os.path.join('data', 'nyc_study.geojson'))
nyc_geodata.columns
```

**R**
```{r}
nyc_geodata = st_read("data/nyc_study.geojson")
names(nyc_geodata)
```

## Disconnected Components (and islands)

New York city consists of several islands; only the Bronx is part of the mainland; Brooklyn and Queens are part of Long Island, plus smaller islands City Island, Roosevelt Island, and the Rockaways.

*This is a problem for the ICAR model, which operates on a fully connected graph (single component)*

* For the NYC analysis paper, we hand edited the map of NYC (in R) to create a fully connected network graph.

* For this notebook, we will restrict out attention to Brooklyn, the largest borough in NYC, which is a single network component.


**Python**
```{python}
brooklyn_geodata = nyc_geodata[nyc_geodata['BoroName']=='Brooklyn'].reset_index(drop=True)
brooklyn_nbs = sa.weights.Rook.from_dataframe(brooklyn_geodata, geom_col='geometry')
plot_spatial_weights(brooklyn_nbs, brooklyn_geodata) 
```

```{python}
print(f'number of components: {brooklyn_nbs.n_components}')
print(f'islands? {brooklyn_nbs.islands}')
print(f'max number of neighbors per node: {brooklyn_nbs.max_neighbors}')
print(f'mean number of neighbors per node: {brooklyn_nbs.mean_neighbors}')
```


**R**
```{r}
brooklyn_geodata = nyc_geodata[nyc_geodata$BoroName=='Brooklyn', ]
brooklyn_nbs = poly2nb(brooklyn_geodata, queen=FALSE)
brooklyn_coords = st_coordinates(st_centroid(brooklyn_geodata['geometry']))
plot(st_geometry(brooklyn_geodata), col='skyblue')
plot(brooklyn_nbs, coords=brooklyn_coords, add=TRUE, pch=20, cex=0.6)
```

```{r}
summary(brooklyn_nbs)
```


## From ICAR to BYM2

* Combines both ICAR component $\phi$ and ordinary random effects $\theta$ as
$$\left( (\sqrt{\, {\rho} / s}\, \ )\,\phi^* + (\sqrt{1-\rho})\,\theta^* \right) \sigma $$

* Parameter $\rho$ answers the question:  how much of the observed variance is spatial?

* Don't need to run analysis, e.g. Moran's I - the model sorts it out for you.

## BYM2 Model:  `bym2.stan`

This file is in directory `stan/bym2.stan`.

**Python**
```{python}
bym2_model_file = os.path.join('stan', 'bym2.stan')

with open(bym2_model_file, 'r') as file:
    contents = file.read()
    print(contents)
```

**R**
```{r}
bym2_model_file = file.path('stan', 'bym2.stan')
cat(readLines(bym2_model_file), sep="\n")
```

## Data Prep

### Get edgeset

- Compute this automatically from `nyc_geodata` spatial geometry component
  + Python package `libpysal`
  + R package `spdep`

**Python**
```{python}
brooklyn_nbs_adj =  brooklyn_nbs.to_adjlist(remove_symmetric=True)
print(type(brooklyn_nbs_adj))
brooklyn_nbs_adj.head(10)
```
```{python}
# create np.ndarray from columns in adjlist, increment indices by 1
j1 = brooklyn_nbs_adj['focal'] + 1
j2 = brooklyn_nbs_adj['neighbor'] + 1
edge_pairs = np.vstack([j1, j2])
edge_pairs
```

**R**
```{r}
brooklyn_nbs_adj = nbs_to_adjlist(brooklyn_nbs)
brooklyn_nbs_adj[1:2, 1:10]
```

### Compute scaling factor `tau`

**Python**
```{python}
tau = get_scaling_factor(brooklyn_nbs)
tau
```

**R**
```{r}
tau = get_scaling_factor(brooklyn_nbs)
tau
```

#### Assemble the input data 

**Python**
```{python}
design_vars = np.array(['pct_pubtransit','med_hh_inc', 'traffic', 'frag_index'])

design_mat = brooklyn_geodata[design_vars].to_numpy()
design_mat[:, 1] = np.log(design_mat[:, 1])
design_mat[:, 2] = np.log(design_mat[:, 2])

pd.DataFrame(data=design_mat).describe()
```

```{python}
bym2_data = {"N":brooklyn_geodata.shape[0],
             "y":brooklyn_geodata['count'].astype('int'),
             "E":brooklyn_geodata['kid_pop'].astype('int'),
             "K":4,
             "xs":design_mat,
             "N_edges": edge_pairs.shape[1],
             "neighbors": edge_pairs,
	     "tau":0.658
}
```

**R**
```{r}
design_mat <- as.data.frame(brooklyn_geodata) %>%
  select(pct_pubtransit, med_hh_inc, traffic, frag_index) %>%
  mutate(med_hh_inc = log(med_hh_inc),
         traffic = log(traffic))
summary(design_mat)
```

```{r}
bym2_data <- list(
  N = nrow(brooklyn_geodata),
  y = as.integer(brooklyn_geodata$count),
  E = as.integer(brooklyn_geodata$kid_pop),
  K = 4,
  xs = design_mat,
  N_edges = ncol(brooklyn_nbs_adj),
  neighbors = brooklyn_nbs_adj,
  tau = tau
)
```

## Fitting the BYM2 Model on the Brooklyn data

#### Model is compiled (as needed) on instantiation 

**Python**
```{python}
bym2_mod = CmdStanModel(stan_file=bym2_model_file)
```


**R**
```{r}
bym2_mod = cmdstan_model(stan_file=bym2_model_file)
```

#### Run the NUTS-HMC sampler, summarize results

**Python**
```{python}
bym2_fit = bym2_mod.sample(data=bym2_data)

bym2_summary = bym2_fit.summary()
bym2_summary.round(2).loc[
  ['beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma', 'rho']]
```


**R**
```{r}
bym2_fit = bym2_mod$sample(data=bym2_data, parallel_chains=cores, refresh=0)
bym2_fit$print(variables = c('beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma', 'rho')) 
```

## Using variational methods to initialize parameters for MCMC

By default, Stan's NUTS-HMC sampler algorithm randomly initializes all model parameters in the range uniform[-2, 2].
When the true parameter value is outside of this range,
starting from the estimates obtained from Stan's variational methods Pathfinder and ADVI
will improve adaptation, resulting in faster warmup and a better sample.

See [Using Variational Estimates to Initialize the NUTS-HMC Sampler](https://mc-stan.org/cmdstanpy/users-guide/examples/VI%20as%20Sampler%20Inits.html) (Python) and [this example on Stan Discourse](https://discourse.mc-stan.org/t/using-pathfinder-or-other-method-to-set-initial-values-for-sampling/34960/8) (R).

For the BYM2 model, the LOO diagnostic, based on Pareto-smoothed importance sampling (PSIS)
is problematic, as the ICAR component in the BYM2 model doesn't factorize, therefore,
when running Pathfinder, we set parameter `psis_resample` to False.


**Python**
```{python}
bym2_pathfinder = bym2_mod.pathfinder(data=bym2_data, psis_resample=FALSE)
bym2_inits = bym2_pathfinder.create_inits()

bym2_inits_fit = bym2_mod.sample(data=bym2_data, inits=bym2_inits)
bym2_inits_summary = bym2_inits_fit.summary()
bym2_inits_summary.round(2).loc[
  ['beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma', 'rho']]
```

```{python}
bym2_summary.round(2).loc[
  ['beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma', 'rho']]

# no summary method for pathfinder draws, summarize var-by-var
print('Pathfinder estimates')
print('beta_intercept', pd.Series(bym2_pathfinder.beta_intercept).describe())
print('sigma', pd.Series(bym2_pathfinder.sigma).describe())
print('rho', pd.Series(bym2_pathfinder.sigma).describe())
```


**R**
```{r}
bym2_pathfinder = bym2_mod$pathfinder(data=bym2_data, psis_resample=FALSE)
bym2_init_fit = bym2_mod$sample(data=bym2_data, parallel_chains=cores, refresh=0, init=bym2_pathfinder)
bym2_init_fit$print(variables = c('beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma', 'rho'))
```
```{r}
bym2_fit$print(variables = c('beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma', 'rho'))
cat('Pathfinder estimates')
bym2_pathfinder$print(variables = c('beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma', 'rho'))
```

## Model Comparison: BYM2 vs ICAR vs. ordinary random effects


#### ICAR model

**Python**
```{python}
pois_icar_mod = CmdStanModel(stan_file=os.path.join(
  'stan', 'poisson_icar.stan'))
pois_icar_fit = pois_icar_mod.sample(data=bym2_data)
pois_icar_summary = pois_icar_fit.summary()
pois_icar_summary.round(2).loc[
  ['beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma']]
```

**R**
```{r}
pois_icar_mod = cmdstan_model(stan_file=file.path(
  'stan', 'poisson_icar.stan')) 
pois_icar_fit = pois_icar_mod$sample(data=bym2_data, parallel_chains=cores, refresh=0)
pois_icar_fit$print(variables = c('beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma'))
```

#### Ordinary random effects model

**Python**
```{python}
pois_re_mod = CmdStanModel(stan_file=os.path.join(
  'stan', 'poisson_re.stan'))
pois_re_fit = pois_re_mod.sample(data=bym2_data)
pois_re_summary = pois_re_fit.summary()
pois_re_summary.round(2).loc[
  ['beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma']]
```

**R**
```{r}
pois_re_mod = cmdstan_model(stan_file=file.path(
  'stan', 'poisson_re.stan')) 
pois_re_fit = pois_re_mod$sample(data=bym2_data, parallel_chains=cores, refresh=0)
pois_re_fit$print(variables = c('beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma'))
```

Which model provides a better fit (on the Brooklyn subset of the data)?

**Python**
```{python}
print('BYM2')
print(bym2_summary.round(2).loc[['sigma', 'rho']])

print('\nPoisson ICAR')
print(pois_icar_summary.round(2).loc[['sigma']])

print('\nPoisson Ordinary random effects')
print(pois_re_summary.round(2).loc[['sigma']])
```

**R**
```{r}
cat("BYM2")
bym2_fit$print(variables = c('sigma', 'rho'))

cat("Poisson ICAR")
pois_icar_fit$print(variables = c('sigma'))

cat("Poisson Ordinary Random Effects")
pois_re_fit$print(variables = c('sigma'))
```

### Visual comparison

#### BYM2 model

**Python**
```{python}
idata_bym2 = az.from_cmdstanpy(
    bym2_fit,
    posterior_predictive="y_rep",
    dims={"betas": ["covariates"]},
    coords={"covariates": design_vars},
    observed_data={"y": bym2_data['y']}
)
idata_bym2
```
```{python}
az_bym2 = az.plot_ppc(idata_bym2, data_pairs={"y":"y_rep"})
az_bym2.set_title("BYM2 model posterior predictive check")
az_bym2
```

**R**
```{r}
y_rep_bym2 <- as_draws_matrix(bym2_fit$draws("y_rep"))
ppc_dens_overlay(brooklyn_geodata$count, y_rep_bym2) +
                 ggtitle("Posterior Predictive Check: BYM2 model")
```


#### ICAR model

**Python**
```{python}
idata_pois_icar = az.from_cmdstanpy(
    pois_icar_fit,
    posterior_predictive="y_rep",
    dims={"betas": ["covariates"]},
    coords={"covariates": design_vars},
    observed_data={"y": bym2_data['y']}
)
idata_pois_icar
```
```{python}
az_pois_icar_ppc_plot = az.plot_ppc(idata_pois_icar, data_pairs={"y":"y_rep"})
az_pois_icar_ppc_plot.set_title("Poisson ICAR model posterior predictive check")
az_pois_icar_ppc_plot
```

**R**
```{r}
y_rep_icar <- as_draws_matrix(pois_icar_fit$draws("y_rep"))
ppc_dens_overlay(brooklyn_geodata$count, y_rep_icar) +
                 ggtitle("Posterior Predictive Check: Poisson ICAR model")
```

#### RE model

**Python**
```{python}
idata_pois_re = az.from_cmdstanpy(
    pois_re_fit,
    posterior_predictive="y_rep",
    dims={"betas": ["covariates"]},
    coords={"covariates": design_vars},
    observed_data={"y": bym2_data['y']}
)
az_pois_re_ppc_plot = az.plot_ppc(idata_pois_re, data_pairs={"y":"y_rep"})
az_pois_re_ppc_plot.set_title("Poisson RE model posterior predictive check")
az_pois_re_ppc_plot
```

**R**
```{r}
y_rep_re <- as_draws_matrix(pois_re_fit$draws("y_rep"))
ppc_dens_overlay(brooklyn_geodata$count, y_rep_re) +
                 ggtitle("Posterior Predictive Check: Poisson RE model")
```
