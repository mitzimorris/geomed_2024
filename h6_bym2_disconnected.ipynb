{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the BYM2 for Disconnected Graphs\n",
    "\n",
    "## Notebook Setup\n",
    "\n",
    "Import all libraries, load the NYC study data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries used in this notebook\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import libpysal as sa\n",
    "import matplotlib\n",
    "import splot as splt\n",
    "from splot.libpysal import plot_spatial_weights \n",
    "import plotnine as p9\n",
    "import arviz as az\n",
    "%matplotlib inline\n",
    "\n",
    "from cmdstanpy import CmdStanModel, cmdstan_path, cmdstan_version, write_stan_json\n",
    "\n",
    "# suppress plotnine warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# setup plotnine look and feel\n",
    "p9.theme_set(\n",
    "  p9.theme_grey() + \n",
    "  p9.theme(text=p9.element_text(size=10),\n",
    "        plot_title=p9.element_text(size=14),\n",
    "        axis_title_x=p9.element_text(size=12),\n",
    "        axis_title_y=p9.element_text(size=12),\n",
    "        axis_text_x=p9.element_text(size=8),\n",
    "        axis_text_y=p9.element_text(size=8)\n",
    "       )\n",
    ")\n",
    "xlabels_90 = p9.theme(axis_text_x = p9.element_text(angle=90, hjust=1))\n",
    "\n",
    "map_theme =  p9.theme(figure_size=(7,6),\n",
    "                 axis_text_x=p9.element_blank(),\n",
    "                 axis_ticks_x=p9.element_blank(),\n",
    "                 axis_text_y=p9.element_blank(),\n",
    "                 axis_ticks_y=p9.element_blank())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_geodata = gpd.read_file(os.path.join('data', 'nyc_study.geojson'))\n",
    "nyc_geodata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_nbs = sa.weights.Rook.from_dataframe(nyc_geodata, geom_col='geometry')\n",
    "print(f'number of components: {nyc_nbs.n_components}')\n",
    "print(f'islands? {nyc_nbs.islands}')\n",
    "print(f'max number of neighbors per node: {nyc_nbs.max_neighbors}')\n",
    "print(f'mean number of neighbors per node: {nyc_nbs.mean_neighbors}')\n",
    "plot_spatial_weights(nyc_nbs, nyc_geodata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup\n",
    "\n",
    "Remove spurious connections between Manhattan, and Brooklyn and Queens,\n",
    "since census tracts in Manhattan are not truly adjacent, although the shapefiles say otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libpysal.weights import W\n",
    "\n",
    "manhattan_indices = nyc_geodata[nyc_geodata['BoroName'] == 'Manhattan'].index\n",
    "brooklyn_indices = nyc_geodata[nyc_geodata['BoroName'] == 'Brooklyn'].index\n",
    "queens_indices = nyc_geodata[nyc_geodata['BoroName'] == 'Queens'].index\n",
    "\n",
    "# Iterate through Manhattan, Brooklyn, and Queens indices to update both neighbors and weights\n",
    "for manhattan_tract in manhattan_indices:\n",
    "    neighbors = nyc_nbs.neighbors[manhattan_tract]\n",
    "    weights = nyc_nbs.weights[manhattan_tract]\n",
    "    clean_neighbors_weights = [(neighbor, weight) for neighbor, weight in zip(neighbors, weights) \n",
    "                             if neighbor not in brooklyn_indices and neighbor not in queens_indices]\n",
    "    \n",
    "    # Unzip the clean neighbors and weights\n",
    "    if clean_neighbors_weights:\n",
    "        clean_neighbors, clean_weights = zip(*clean_neighbors_weights)\n",
    "    else:\n",
    "        clean_neighbors, clean_weights = [], []\n",
    "    \n",
    "    nyc_nbs.neighbors[manhattan_tract] = list(clean_neighbors)\n",
    "    nyc_nbs.weights[manhattan_tract] = list(clean_weights)\n",
    "\n",
    "for brooklyn_tract in brooklyn_indices:\n",
    "    neighbors = nyc_nbs.neighbors[brooklyn_tract]\n",
    "    weights = nyc_nbs.weights[brooklyn_tract]\n",
    "    clean_neighbors_weights = [(neighbor, weight) for neighbor, weight in zip(neighbors, weights) \n",
    "                             if neighbor not in manhattan_indices]\n",
    "    \n",
    "    if clean_neighbors_weights:\n",
    "        clean_neighbors, clean_weights = zip(*clean_neighbors_weights)\n",
    "    else:\n",
    "        clean_neighbors, clean_weights = [], []\n",
    "\n",
    "    nyc_nbs.neighbors[brooklyn_tract] = list(clean_neighbors)\n",
    "    nyc_nbs.weights[brooklyn_tract] = list(clean_weights)\n",
    "\n",
    "for queens_tract in queens_indices:\n",
    "    neighbors = nyc_nbs.neighbors[queens_tract]\n",
    "    weights = nyc_nbs.weights[queens_tract]\n",
    "    clean_neighbors_weights = [(neighbor, weight) for neighbor, weight in zip(neighbors, weights) \n",
    "                             if neighbor not in manhattan_indices]\n",
    "    \n",
    "    if clean_neighbors_weights:\n",
    "        clean_neighbors, clean_weights = zip(*clean_neighbors_weights)\n",
    "    else:\n",
    "        clean_neighbors, clean_weights = [], []\n",
    "\n",
    "    nyc_nbs.neighbors[queens_tract] = list(clean_neighbors)\n",
    "    nyc_nbs.weights[queens_tract] = list(clean_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, re-create the W object with the updated neighbors and weights\n",
    "clean_nyc_nbs = W(nyc_nbs.neighbors, nyc_nbs.weights)\n",
    "plot_spatial_weights(clean_nyc_nbs, nyc_geodata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute components\n",
    "components = clean_nyc_nbs.component_labels\n",
    "print(f'Number of components: {len(set(components))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BYM2 Model for Disconnected Graphs and Islands\n",
    "\n",
    "This model requires the following modifications:\n",
    "\n",
    "* ICAR component accounts for singletons as well as disconnected components, therefore need list of singleton node ids\n",
    "\n",
    "* Instead of single scaling factor, per-region scaling factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bym2_islands_model_file = os.path.join('stan', 'bym2_islands.stan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bym2_islands_model_file, 'r') as file:\n",
    "    contents = file.read()\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "### Observed predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_vars = np.array(['pct_pubtransit','med_hh_inc', 'traffic', 'frag_index'])\n",
    "\n",
    "design_mat = nyc_geodata[design_vars].to_numpy()\n",
    "design_mat[:, 1] = np.log(design_mat[:, 1])\n",
    "design_mat[:, 2] = np.log(design_mat[:, 2])\n",
    "\n",
    "pd.DataFrame(data=design_mat).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_nbs_adj =  clean_nyc_nbs.to_adjlist(remove_symmetric=True)\n",
    "\n",
    "# create np.ndarray from columns in adjlist, Stan indices count from 1 (like R - Python is 0-based).\n",
    "j1 = nyc_nbs_adj['focal'] + 1\n",
    "j2 = nyc_nbs_adj['neighbor'] + 1\n",
    "edge_pairs = np.vstack([j1, j2])\n",
    "\n",
    "singleton_ids = clean_nyc_nbs.islands\n",
    "for n in range(0, len(singleton_ids)) :\n",
    "    singleton_ids[n] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute scaling factor `tau`\n",
    "\n",
    "Scaling factor for singletons is 1, Scaling factor for multi-node component is computed as before\n",
    "\n",
    "* Compute cardinality of each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comp_ids = clean_nyc_nbs.component_labels\n",
    "(comp_id, counts) = np.unique(comp_ids, return_counts = True)\n",
    "comp_id, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get subset of regions for multi-node components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_geodata['comp_id'] = comp_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taus = np.ones(len(counts))\n",
    "for id in range(len(counts)):\n",
    "    comp = nyc_geodata[nyc_geodata['comp_id']==id]\n",
    "    comp_nbs = sa.weights.Rook.from_datafram(comp, geom_col='geometry')\n",
    "    taus[i] = get_scaling_factor(comp_nbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute scaling_factor  (or do this in R)\n",
    "\n",
    "* Scale each region according to scaling factor for the component they belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = np.array([0.73, 1.0, 0.93, 1.3, 0.25, 1.0, 1.19, 1.0, 0.45])\n",
    "taus = tau[clean_nyc_nbs.component_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asesmble data into list/dict object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bym2_islands_data = {\n",
    "    \"N\":nyc_geodata.shape[0],\n",
    "    \"y\":nyc_geodata['count'].astype('int'),\n",
    "    \"E\":nyc_geodata['kid_pop'].astype('int'),\n",
    "    \"K\":4,\n",
    "    \"xs\":design_mat,\n",
    "    \"N_edges\": edge_pairs.shape[1],\n",
    "    \"neighbors\": edge_pairs,\n",
    "    \"taus\": taus,\n",
    "    \"N_singletons\" : len(singleton_ids),\n",
    "    \"singletons\":singleton_ids\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the BYM2 model\n",
    "\n",
    "#### Model is compiled (as needed) on instantiation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bym2_islands_mod = CmdStanModel(stan_file=bym2_islands_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Pathfinder to get initial parameter values\n",
    "\n",
    "see CmdStanPy notebook: [Using Variational Estimates to Initialize the NUTS-HMC Sampler](https://mc-stan.org/cmdstanpy/users-guide/examples/VI%20as%20Sampler%20Inits.html#Using-Variational-Estimates-to-Initialize-the-NUTS-HMC-Sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bym2_islands_pathfinder = bym2_islands_mod.pathfinder(data=bym2_islands_data)\n",
    "\n",
    "param_inits = bym2_islands_pathfinder.create_inits()\n",
    "\n",
    "bym2_islands_fit_pathfinder_inits = bym2_islands_mod.sample(\n",
    "    data=bym2_islands_data, inits=param_inits, iter_warmup=3000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bym2_islands_fit_pathfinder_inits.summary().round(2).loc[\n",
    "  ['beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma', 'rho']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does pathfinder help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bym2_islands_fit_default_inits = bym2_islands_mod.sample(\n",
    "    data=bym2_islands_data, iter_warmup=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bym2_islands_fit_default_inits.summary().round(2).loc[\n",
    "  ['beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma', 'rho']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata_bym2_islands = az.from_cmdstanpy(\n",
    "    bym2_islands_fit_default_inits,\n",
    "    posterior_predictive=\"y_rep\",\n",
    "    dims={\"betas\": [\"covariates\"]},\n",
    "    coords={\"covariates\": design_vars},\n",
    "    observed_data={\"y\": bym2_islands_data['y']}\n",
    ")\n",
    "idata_bym2_islands\n",
    "\n",
    "az.plot_ppc(idata_bym2_islands, data_pairs={\"y\":\"y_rep\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
