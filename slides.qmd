---
title: Spatial Modeling in Stan
author: Mitzi Morris
institute: Stan Development Team
date: Sept 9, 2024
format:
  beamer:
    syntax-definitions:
      - "theming/stan.xml"
    highlight-style: tango
    aspectratio: 169
    pdf-engine: pdflatex
    theme: metropolis
    include-in-header: preamble.tex
    navigation: horizontal
    dev: png 
    linkcolor: "DarkBlue"
    urlcolor: "DarkBlue"
---

# Overview
\vspace*{-20pt}

### Workshop Goal

Learn how to use existing models and develop new ones.

### Outline

* About Stan

* Stan Workflow

* ICAR Model for Spatial Smoothing

* BYM2 Model for Spatial and Instrinsic Random Effects

* Extensions to the BYM2 Model


# What is Stan?

\tightlist
* A Probabilistic Programming Language
  + A Stan program defines a probability model

\vspace*{4pt}
* A Collection of Inference Algorithms
  + Exact Bayesian inference using MCMC (NUTS-HMC)
  + Aproximate Bayesian inference using ADVI, Pathfinder
  + Point estimates using optimization, Laplace approximation

\vspace*{4pt}
* Interfaces and Analysis Tools
  + R: [CmdStanR](https://mc-stan.org/cmdstanr/), [Posterior](https://mc-stan.org/posterior/), [Bayesplot](https://mc-stan.org/bayesplot/), [LOO](https://mc-stan.org/loo/) (Also:  RStan, RStanARM, BRMS)
  + Python: [CmdStanPy](https://mc-stan.org/cmdstanpy/), [ArviZ](https://python.arviz.org/en/stable/)
  + Julia: [Stan.jl](https://stanjulia.github.io/Stan.jl/v10.8/), [ArviZ.jl](https://julia.arviz.org/ArviZ/stable/)
  + Command shell: [CmdStan](https://mc-stan.org/docs/cmdstan-guide/)


# Why Stan?

\tightlist

* **Goal**: Develop multi-level models for real-world applications

\vspace*{8pt}
* *Problem*:  Need descriptive power, clarity of BUGS
* *Solution*: Compile a domain-specific language

 . . .

\vspace*{8pt}
* *Problem*:  Pure directed graphical language inflexible
* *Solution*: Imperative probabilistic programming language

 . . .

\vspace*{8pt}
* *Problem*:  Heterogeneous user base
* *Solution*: Many interfaces (R, Python, Julia, Mathematica, MATLAB), domain-specific examples, case studies, and field guides

 . . .

\vspace*{8pt}
* *Problem*:  Restrictive licensing limits use
* *Solution*: Code and doc open source (BSD, CC-BY)

# Why Stan?

\tightlist
* **Goal**: Robust, fully Bayesian inference.

\vspace*{8pt}
* *Problem*: Gibbs and Metropolis too slow (diffusive)
* *Solution*: Hamiltonian Monte Carlo (flow of Hamiltonian dynamics)

 . . .

\vspace*{8pt}
* *Problem*:  Need to tune parameters for HMC
* *Solution*: Tune step size and estimate mass matrix during warmup;\
determine number of steps on-the-fly (NUTS)

 . . .

\vspace*{8pt}
* *Problem*:  Need gradients of log posterior for HMC
* *Solution*: Reverse-mode algorithmic differentiation

 . . .

\vspace*{8pt}
* *Problem*:  Need unconstrained parameters for HMC
* *Solution*: Variable transforms w. Jacobian determinants

# NUTS-HMC vs.\ Gibbs and Metropolis

\includegraphics[width=0.9\textwidth]{img/nuts-vs.pdf}

* Plot shows 2 dimensions of highly correlated 250-dim normal
* **1,000,000 draws** from Metropolis and Gibbs (thin to 1000)
* **1000 draws** from NUTS (no thinning); 1000 independent draws
\fontsize{9pt}{9.4}\selectfont
* [*The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo*](http://jmlr.org/papers/v15/hoffman14a.html), fig. 7
\normalsize

# MCMC Sampling Efficiency
\tightlist
* MCMC samples have autocorrelation

* $N_{\mbox{\footnotesize eff}}$ is the number of independent samples with the same estimation power\
as the $N$ autocorrelated samples.

* When comparing algorithms, compare $N_{\mbox{\footnotesize eff}}$ **per second**

\vspace{0.05in}
* Gibbs and Metropolis have
  + High iterations per second but low effective samples per iteration
  + Both are weak when there is high correlation among the parameters in the posterior

\vspace{0.05in}
* Hamiltonian Monte Carlo \& NUTS
  + Fewer iterations per second than Gibbs or Metropolis
  + But *much* higher $N_{\mbox{\footnotesize eff}}$ per second
  + More robust to highly correlated or differently scaled parameters.

# Other Supported Inference Algorithms

Faster, more scalable methods
\vspace{0.1in}

\tightlist
* Variational Inference\
Generate samples from a variational approximation to the posterior distribution
   + [ADVI](https://www.jmlr.org/papers/volume18/16-107/16-107.pdf) - use stochastic gradient ascent
   + Pathfinder - follow quasi-Newton optimization path


\vspace{0.05in}
* Optimization: find posterior mode
   + MLE - maximum likelihood estimate (of the data given the parameters)
   + MAP - maximum a posteriori estimate (of the value of the posterior density\
on the unconstrained scale)
   + Laplace approximation - obtain samples from posterior mode


# Stan Toolset

* Programming language interfaces:  CmdStanR, CmdStanPy, Stan.jl, (MatLabStan)
  + Core methods for model compilation, doing inference, accessing results

\vspace{0.05in}
* Tools for visualization and validation:  Bayesplot, LOO, ArviZ, Arviz.jl
  + Evaluate the goodness of fit; make plots to communicate findings
  
\vspace{0.05in}
* Higher Level Interface for R: BRMS - Bayesian Regression Models in Stan
  + Model specification via extended R formula, data supplied as R dataframe
  + BRMS translates formula to Stan, creates list of inputs from dataframe,\
runs NUTS-HMC sampler, checks inferences.


\vspace{0.05in}
* BridgeStan - algorithm development and exploration
  + Evaluate the log posterior density at a point
  + Constrain/unconstrain model parameters


# The Stan Language

\tightlist
* A Stan program
  + Declares data and (constrained) parameter variables
  + Defines log posterior (or penalized likelihood)
  + Computes quantities of interest

* Syntax
  + Influenced by BUGS (plus Java/C++ punctuation)
  + Explicit variable types (like Java/C++, not like Python, R)
  + Named program blocks - distinguish between data and parameters (not like BUGS)
  + Control flow:  `if` statements - dynamic branch points (more powerful than BUGS)

* Mathematical operations
  + Stan language includes a very large set of probability distributions\
and mathematical functions from Stan's math library
  + Efficient, vectorized (almost always)

# Stan Example: Laplace's Model of Birth Rate by Sex

\vspace{-0.1in}
Laplace's data on live births in Paris from 1745--1770:

\begin{center}\small
\begin{tabular}{c|c}
{\slshape sex} & {\slshape live births}
\\ \hline
female & 241\,945
\\
male & 251\,527
\end{tabular}
\end{center}

\vspace*{0.1in}

* *Question 1* (Estimation): What is the birth rate of boys vs. girls?

* *Question 2* (Event Probability): Is a boy more likely to be born than a girl?


\vspace*{0.15in}
Laplace computed this analytically.  Let's use Stan's NUTS-HMC sampler instead.


# Laplace's Model in Stan

\fontsize{9pt}{9.4}\selectfont
```stan
transformed data {
  int male = 251527;
  int female = 241945;
}
parameters {
  real<lower=0, upper=1> theta;
}
model {
  male ~ binomial(male + female, theta);
}
generated quantities {
  int<lower=0, upper=1> theta_gt_half = (theta > 0.5);
}
```
\normalsize


# Laplace's Answer
\vspace{-0.1in}

\fontsize{9pt}{9.4}\selectfont
```
births_model = cmdstan_model("laplace.stan")   # compile model
births_fit = births_model$sample()             # run inference algorithm
as.data.frame(births_fit$summary())            # manage results

      variable       mean     median       sd         q5         q95

         theta       0.51       0.51 0.000725      0.509       0.511
 theta_gt_half       1.00       1.00 0.000000      1.000       1.000
```
\normalsize

\vspace{0.1in}
* *Question 1* (Estimation): What is the birth rate of boys vs. girls?\
$\theta$ is 90\% certain to lie in $(0.509, 0.511)$

* *Question 2* (Event Probability): Is a boy more likely to be born than a girl?\
Laplace "morally certain" boys more prevalent


# Stan Program File

A Stan program consists of one or more named program blocks, strictly ordered

\fontsize{9pt}{9.4}\selectfont
```stan
functions {
  // declare, define functions
} data {
  // declare input data
} transformed data {
   // transform inputs, define program data
} parameters {
   // declare (continuous) parameters
} transformed parameters {
   // define derived parameters
} model {
   // compute the log joint distribution
} generated quantities {
   // define quantities of interest
}
```
\normalsize

# Stan Program Blocks - Execution During Sampling

* `data`, `transformed data` blocks - executed once on startup

* `parameters` - 
   + on startup: initialize parameters
   + at every step of inference algorithm: validate constraints

* `transformed parameters`, `model` blocks - executed every *step* of the sampler

* `generated quantities` - executed every *iteration* of the sampler

\vspace*{4pt}
* After every sampler interation, program outputs current values of all variables\
in parameters, transformed parameters, and generated quantities blocks.

# The Stan Language

\tightlist
* Variables have strong, static types
  + Strong:  only values of that type will be assignable to the variable,\
(type promotion allowed, e.g., `int` to `real`, `complex`)
  + Static: variable type is constant throughout program
  + Types: [vector, row_vector, matrix](https://mc-stan.org/docs/reference-manual/types.html#vector-and-matrix-data-types), [complex](https://mc-stan.org/docs/reference-manual/types.html#complex-numerical-data-type),
[array](https://mc-stan.org/docs/reference-manual/types.html#array-data-types.section), and [tuple](https://mc-stan.org/docs/reference-manual/types.html#tuple-data-type)
  + Variables can be declared with constraints on [upper and/or lower bounds](https://mc-stan.org/docs/reference-manual/types.html#variable-types-vs.-constraints-and-sizes)
  
* Motivation
  + Static types make programs easier to comprehend, debug, and maintain
  + Programming errors can be detected at compile-time
  + Constrained types catch runtime errors

\vspace*{4pt}
\fontsize{9pt}{9.4}\selectfont
```stan
  int<lower=0> N, N_time;                     // constraint applies to both N, N_time
  vector[3]<lower=0> sigma;
  array[3] matrix[M, N] some_xs;
  array[N] int<lower=0, upper=N_time> time;  // use arrays for structured int data 
```
\normalsize

# The Stan Language

The `model` block

\tightlist
* defines the joint log probability density function given parameters
* this function is the sum of all distribution and log probability increment statements
in the model block

\vspace*{8pt}
[Distribution statements](https://mc-stan.org/docs/reference-manual/statements.html#distribution-statements.section)
\fontsize{9pt}{9.4}\selectfont
```stan
y ~ normal(mu, sigma);
mu ~ normal(0, 10);
sigma ~ normal(0, 1);
```
\normalsize

\vspace*{8pt}
[Log probability increment statements](https://mc-stan.org/docs/reference-manual/statements.html#log-probability-increment-vs.-distribution-statement)
\fontsize{9pt}{9.4}\selectfont
```stan
target += normal_lupdf(y | mu, sigma);
target += normal_lupdf(mu | 0, 10);
target += normal_lupdf(sigma | 0, 1);
```
\normalsize



# Processing Steps

* Compile model
  + Stan compiler translates Stan file to C++ file
  + C++ file is compiled to executable program, via GNU Make
  + *prerequisites: a C++17 compiler, GNU-Make*\
*both CmdStanPy and CmdStanR have utilities to install these*

* Run inference algorithm
  + Interfaces run the compiled executable (as a subprocess) and\
manage the per-chain outputs (modified CSV format files)

* Get results via interface methods
  + Parse CSV outputs into in-memory object
  + Access individual parameters and quantities of interest
  + Run summary and diagnostic reports


# Install Stan


*  If you don't already have Stan running on your machine, 10 minutes to get it working now.
   + *If you already have Stan running on your machine, please help your neighbor.*

\vspace*{8pt}
* See handout:  `h1_install_stan.html`


# Notebook: Spatial Data for Python and R

Dataset taken from
[Bayesian Hierarchical Spatial Models: Implementing the Besag York Mollié Model in Stan](https://www.sciencedirect.com/science/article/pii/S1877584518301175).

\tightlist
* Aggregated counts of car vs. child traffic accidents, localized to US Census tract.  Per-tract data includes:
  + raw counts of accidents, population
  + measures of foot traffic, car traffic
  + socio-economic indicators:  median income, neighborhood transiency

* Handouts
  * `h2_spatial_data.html`
  * R version `h2_spatial_data.Rmd`
  * Python version: `h2_spatial_data.ipynb`



# Stan Model Building Workflow

\vspace*{-0.2in}

When writing a Stan model, as when writing any other computer program,\
*the fastest way to success is to go slowly.*

\vspace*{8pt}

* Incremental development
   + Write a (simple) model
   + Fit the model to data (either simulated or observed)
   + Check the fit 

* Then modify *(stepwise)* and repeat

* Compare successive models


# Base Model:  Poisson Regression

##### Data

- `N` - the number of census tracts
- `y` - the array of observed outcomes - accidents per tract
- `E` - the population per tract ("exposure")
- `K` - the number of predictors
- `xs` - the N x K data matrix of predictors

##### Regression Parameters

- `beta0` - global intercept
- `betas` - a K-length vector of regression co-efficients



# Base Model:  Poisson Regression

\vspace*{-0.2in}

##### Distribution statement (likelihood)

\fontsize{9pt}{9.4}\selectfont
```stan
  y ~ poisson_log(log(E) + beta0 + X * betas);
```
\normalsize

\vspace*{4pt}
- **`poisson_log`** distribution uses the log rate as a parameter
   + don't need to exponentiate, better numerical stability

\vspace*{4pt}
- **`poisson_log_rng`** pseudo-random number generator function (PRNG function)
  +  [PRNG functions](https://mc-stan.org/docs/stan-users-guide/user-functions.html#functions-acting-as-random-number-generators) used to replicate, simulate data.

  + Poisson variate is an **integer**,  largest integer value is $2^{30}$,
argument to `poisson_log_rng` must be less than $30 \log 2$, $\approx 28$



# Base Stan Program

\fontsize{9pt}{9.4}\selectfont
```stan
data {
  int<lower=0> N;
  array[N] int<lower=0> y; // count outcomes
  vector<lower=0>[N] E; // exposure
  int<lower=1> K; // num covariates
  matrix[N, K] xs; // design matrix
}
transformed data {
  vector[N] log_E = log(E);
}
parameters {
  real beta0; // intercept
  vector[K] betas; // covariates
}
model {
  y ~ poisson_log(log_E + beta0 + xs * betas);  // likelihood
  beta0 ~ std_normal(); betas ~ std_normal();   // priors
}
```
\normalsize

# Posterior Predictive Checks

* The posterior predictive distribution is the distribution over new observations given previous observations.

* In the absence of new observations, we can simulate new observations, `y_rep` in the `generated quantities` block.

$y^{\textrm{rep}}$ of the original data set $y$ given model parameters
$\theta$ is defined by
$$
p(y^{\textrm{rep}} \mid y)
= \int p(y^{\textrm{rep}} \mid \theta)
       \cdot p(\theta \mid y) \, \textrm{d}\theta.
$$


# Base Stan Program - Generated Quantities Block

\fontsize{9pt}{9.4}\selectfont
```stan
generated quantities {
  array[N] int y_rep;
  vector[N] log_lik;
  { // local block variables not recorded
    vector[N] eta = log_E + beta0 + xs * betas;
    if (max(eta) > 26) { // avoid overflow in poisson_log_rng
      print("max eta too big: ", max(eta));
      for (n in 1:N) {
        y_rep[n] = -1;  log_lik[n] = -1;
      }
    } else {
      for (n in 1:N) {
        y_rep[n] = poisson_log_rng(eta[n]);
        log_lik[n] = poisson_log_lpmf(y[n] | eta[n]);
      }
    }
  }
}
```
\normalsize

# Notebook: Stan Model Building Workflow

* Start with base model
* Best practice one:  mean-center, scale predictor data
* Refine model: add a random effects component
* Check fits, compare models

* Handouts
  + `h3_stan_workflow.html`
  + R version `h3_stan_workflow.Rmd`
  + Python version: `h3_stan_workflow.ipynb`

# Spatial Smoothing For Areal Data

\vspace*{-0.1in}
Study goal: explain *rate* of traffic accidents  (count / kid_pop)

\vspace*{8pt}
\tightlist
* Counts of rare events in small-population regions are noisy
  + solution: pool information from *neighboring* areas

\vspace*{4pt}
* Besag, 1973, 1974:  Conditional Auto-Regressive Model (CAR), and\
**Intrinsic Conditional Auto-Regressive** (ICAR) model

\vspace*{4pt}
* CAR model has parameter $\alpha$ for the amount of spatial dependence;
  + CAR model requires computing matrix determinants
  + cubic operation (calculated at every *step* of the sampler)

\vspace*{4pt}
* ICAR simplification: let $\alpha == 1$ (complete spatial dependence)
  + ICAR model computes pairwise distance between neighbors
  + quadratic operation

# Neighbor Relationship

\vspace*{-0.1in}
\tightlist
* The binary neighbor relationship (written $i \sim j$ where $i \neq j$) is
  + $1$ if regions $n_i$ and $n_j$ are neighbors\
  + $0$ otherwise

\vspace*{8pt}
* Many possible definitions of "neighbor"
  +  'rook' - areas share a bounding line
  +  'queen' - areas share a boundary point
  +  *transit patterns* - residents travel (frequently, directly) between areas

\vspace*{8pt}
* For CAR models, neighbor relationship is:
  + symmetric - if $i \sim j$ then $j \sim i$
  + not reflexive - a region is not its own neighbor ($i\ \cancel{\sim}\ i$)

# From Map to Graph

\vspace*{-0.1in}
The neighborhood network can be represented as either a matrix of a graph.

\vspace*{4pt}
\tightlist
- $N \times N$ matrix
  + Entries $(i,\ j)$ and $(j,\ i)$ are 1 when regions $n_i$ and $n_j$ are neighbors, 0 otherwise

\vspace*{8pt}
- Undirected graph: regions are vertices, pairs of neighbors are edges
  + Encoded as a set of *edges* - 2 column matrix, each row is a pair of neighbors $({n_i}, {n_j})$

\vspace*{8pt}
- If the number of neighbors per area is relatively small compared to the number of areas,
the adjacency matrix will be sparse

\vspace*{8pt}
- Computing with an edgeset is more efficient
  + data structure requires less storage
  + adjacent nodes in-memory adjacent


# Intrinsic Conditional Auto-Regressive (ICAR) Model

* Conditional specification: multivariate normal random vector $\mathbf{\phi}$
where each ${\phi}_i$ is conditional on the values of its neighbors

* Joint specification rewrites to _Pairwise Difference_:
$$ p(\phi) \propto \exp \left\{ {- \frac{1}{2}} \sum_{i \sim j}{({\phi}_i - {\phi}_j)}^2 \right\} $$

  + centered at 0, assuming common variance for all elements of $\phi$.

* Each ${({\phi}_i - {\phi}_j)}^2$ contributes a
penalty term based on the distance between the values of neighboring regions

\tightlist
* $\phi$ is non-identifiable, constant added to $\phi$ washes out of ${\phi}_i - {\phi}_j$
  + sum-to-zero constraint centers $\phi$

# Stan ICAR Model

$$ p(\phi) \propto \exp \left\{ {- \frac{1}{2}} \sum_{i \sim j}{({\phi}_i - {\phi}_j)}^2 \right\} $$

Use Stan's vectorized operations to compute log probability density

\fontsize{9pt}{9.4}\selectfont
```
   target += -0.5 * dot_self(phi[node1] - phi[node2]);
```
\normalsize
\vspace{0.2in}

Encode neighbor information as graph edgeset, i.e. pairs of indices for neighbors $i, j$:
\fontsize{9pt}{9.4}\selectfont
```
  int<lower = 0> N;  // number of areal regions
  int<lower = 0> N_edges;  // number of neighbor pairs
  array[2, N_edges] int<lower = 1, upper = N> neighbors; // columnwise adjacent
```
\normalsize

# Stan ICAR Model Implementation

\fontsize{9pt}{9.4}\selectfont
```
functions {
  real standard_icar_lpdf(vector phi, array[ , ] int adjacency, real epsilon) {
    return -0.5 * dot_self(phi[adjacency[1]] - phi[adjacency[2]])
      + normal_lupdf(sum(phi) | 0, epsilon * rows(phi));
 }
}
data {
  int<lower=0> N;
  int<lower=0> N_edges;
  array[2, N_edges] int<lower=1, upper=N> neighbors;
}
parameters {
  vector[N] phi;
}
model {
  phi ~ standard_icar(neighbors, 0.001);
}
```
\normalsize


# Notebook: ICAR model

* Add a spatial random effects component to the base model.
* Check fits, compare ICAR, ordinary random effects models

* Handouts
  + `h4_icar.html`
  + R version `h4_icar.Rmd`
  + Python version: `h4_icar.ipynb`


# Besag York Mollié (1991) BYM Model

* Acount for both spatial and heterogenous variation

* Lognormal Poisson regression ${\eta}_i = \mu + x \beta + \phi + \theta$

* $\mu$ is the fixed intercept.

* $x$ is the design matrix, $\beta$ is vector of regression coefficients.

* $\phi$ is an ICAR spatial component

* $\theta$ is an vector of ordinary random-effects components.

* MCMC samplers require strong hyperpriors on $\phi$ and $\theta$.
    + Bernardinelli et. al. (1995): hyperpriors depend on average number of neighbors, i.e., hyperpriors depend on data.


# BYM2 model:  Riebler et al, 2016

* Model combination of spatial and non-spatial components $\phi + \theta$ as
$$\left( (\sqrt{\, {\rho} / s}\, \ )\,\phi^* + (\sqrt{1-\rho})\,\theta^* \right) \sigma $$
where:
  + $\sigma\geq 0$ is the overall standard deviation.
  + $\rho \in [0,1]$ - proportion of spatial variance.
  + $\phi^*$ is the ICAR component.
  + $\theta^* \sim N(0, 1)$ is the vector of ordinary random effects
  + $s$ is a scaling factor s.t. $\operatorname{Var}(\phi_i) \approx 1$; $s$ is data.

\tightlist
* Follows a series of improvements, notably Leroux 2000
  + single scale parameter plus mixing parameter


# BYM2 model scaling factor

Besag 1974: joint specification of $\phi$ is multivariate normal, zero-centered
$$\phi \sim \mathrm{N}(0, Q^{-1}).$$

* The variance of $\phi$ is specified as the precision matrix $Q$,
  + inverse of the covariance matrix $\Sigma$, i.e. $\Sigma = Q^{-1}$

* The variance of $\theta$ is a scalar.

* To clearly interpret $\sigma$ (variance of combined components), for each $i$, $\operatorname{Var}(\phi_i) \approx \operatorname{Var}(\theta_i) \approx 1$.

* Scale the proportion of variance $\rho$ contributed by $\phi$ 

# Computing ICAR scaling factor

\vspace{-0.1in}
Goal: for each $i$, $\operatorname{Var}(\phi_i) \approx 1$ \
"the geometric mean of the average marginal variance of the areal units is $1$"

\vspace*{8pt}
\tightlist
1. Construct Q, the precision matrix for to the neighborhood network\
convert the edgelist to an adjacency matrix

2. Add a small amount of noise to the diagonal elements of Q (positive definite)

3. Compite Q_inv, the covariance matrix - **computationally expensive**

4. Compute the geometric mean of the diagonal elements of Q_inv\
`tau = exp(mean(log(x)))`

\vspace*{8pt}
*The scaling factor comes in as data* - must be computed in either Python or R

# BYM2 Stan Implementation, Functions Block

\fontsize{9pt}{9.4}\selectfont
```stan
functions {
  /**
   * Compute ICAR, wse soft-sum-to-zero constraint for identifiability
   *
   * @param phi vector of varying effects
   * @param adjacency parallel arrays of indexes of adjacent elements of phi
   * @param epsilon allowed variance for soft centering
   * @return ICAR log probability density
   * @reject if the the adjacency matrix does not have two rows
   */
  real standard_icar_lpdf(vector phi, array[ , ] int adjacency, real epsilon) {
    if (size(adjacency) != 2)
      reject("require 2rows for adjacency array;",
             " found rows = ", size(adjacency));
    return -0.5 * dot_self(phi[adjacency[1]] - phi[adjacency[2]])
      + normal_lupdf(sum(phi) | 0, epsilon * rows(phi));
  }
}
```
\normalsize

# BYM2 Stan Implementation, Data Block

\fontsize{9pt}{9.4}\selectfont
```stan
data {
  int<lower=0> N;
  array[N] int<lower=0> y; // count outcomes
  vector<lower=0>[N] E; // exposure
  int<lower=1> K; // num covariates
  matrix[N, K] xs; // design matrix

  // spatial structure
  int<lower = 0> N_edges;  // number of neighbor pairs
  array[2, N_edges] int<lower = 1, upper = N> neighbors;  // columnwise adjacent

  real tau; // scaling factor
}
```

# BYM2 Stan Implementation, Parameters, Model Blocks

\fontsize{9pt}{9.4}\selectfont
```stan
parameters {
  real beta0; // intercept
  vector[K] betas; // covariates
  real<lower=0, upper=1> rho; // proportion of spatial variance
  vector[N] phi; // spatial random effects
  vector[N] theta; // heterogeneous random effects
  real<lower = 0> sigma;  // scale of combined effects
}
transformed parameters {
  vector[N] gamma = (sqrt(1 - rho) * theta + sqrt(rho / tau) * phi);  // BYM2
}
model {
  y ~ poisson_log(log_E + beta0 + xs_centered * betas + gamma * sigma);
  rho ~ beta(0.5, 0.5);
  phi ~ standard_icar(neighbors, 0.001);
  // std_normal() prior on all other params
```

# Notebook: BYM2 model

* Implement the BYM2 model.
* Check fits, compare to ICAR, ordinary random effects models

* Handouts
  + `h5_bym2.html`
  + R version `h5_bym2.Rmd`  (or `h5_bym2.qmd`)
  + Python version: `h5_bym2.ipynb`



# References


* \href{https://mc-stan.org/docs/stan-users-guide/index.html}{Stan User's Guide}
* \href{https://bob-carpenter.github.io/stan-getting-started/stan-getting-started.html}{Getting Started with Bayesian Statistics}
* \href {https://www.usgs.gov/faqs/what-geographic-information-system-gis}{Geographic Information System}(GIS)
* \href{https://github.com/mitzimorris/ljubljiana_lecture/blob/main/data_prep_spatial_maps.ipynb}{Map-making with Plotnine}

* Course notes: *Software for MCMC* from Odd Kolbjørnsen, Spring 2023, Oslo Uni\
\fontsize{9pt}{9.4}\selectfont
www.uio.no/studier/emner/matnat/math/STK4051/v23/timeplan/lecture_12_softwareformcmc.pdf
\normalsize
