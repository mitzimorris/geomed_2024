{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stan Model Building Workflow\n",
    "\n",
    "When writing a Stan model, as when writing any other computer program,\\\n",
    "*the fastest way to success is to go slowly.*\n",
    "\n",
    "* Incremental development\n",
    "   + Write a (simple) model\n",
    "   + Fit the model to data (either simulated or observed)\n",
    "   + Check the fit \n",
    "\n",
    "* Then modify *(stepwise)* and repeat\n",
    "\n",
    "* Compare successive models\n",
    "\n",
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries used in this notebook\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import libpysal as sa\n",
    "import matplotlib\n",
    "import splot as splt\n",
    "import plotnine as p9\n",
    "import arviz as az\n",
    "%matplotlib inline\n",
    "\n",
    "from cmdstanpy import CmdStanModel, cmdstan_path, cmdstan_version\n",
    "\n",
    "# suppress plotnine warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# setup plotnine look and feel\n",
    "p9.theme_set(\n",
    "  p9.theme_grey() + \n",
    "  p9.theme(text=p9.element_text(size=10),\n",
    "        plot_title=p9.element_text(size=14),\n",
    "        axis_title_x=p9.element_text(size=12),\n",
    "        axis_title_y=p9.element_text(size=12),\n",
    "        axis_text_x=p9.element_text(size=8),\n",
    "        axis_text_y=p9.element_text(size=8)\n",
    "       )\n",
    ")\n",
    "xlabels_90 = p9.theme(axis_text_x = p9.element_text(angle=90, hjust=1))\n",
    "\n",
    "map_theme =  p9.theme(figure_size=(7,6),\n",
    "                 axis_text_x=p9.element_blank(),\n",
    "                 axis_ticks_x=p9.element_blank(),\n",
    "                 axis_text_y=p9.element_blank(),\n",
    "                 axis_ticks_y=p9.element_blank())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the NYC study data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_geodata = gpd.read_file(os.path.join('data', 'nyc_study.geojson'))\n",
    "print(nyc_geodata.columns)\n",
    "print(nyc_geodata['BoroName'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble the input data\n",
    "\n",
    "The data block of the model declares variables:\n",
    "\n",
    "- `N` - the number of census tracts\n",
    "- `y` - the array of observed outcomes - accidents per tract\n",
    "- `E` - the population per tract (\"exposure\")\n",
    "- `K` - the number of predictors\n",
    "- `xs` - the N x K data matrix of predictors\n",
    "\n",
    "The predictors from the study data are columns: `pct_pubtransit`,`med_hh_inc`, `traffic`, `frag_index`.\n",
    "\n",
    "**Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_vars = np.array(['pct_pubtransit','med_hh_inc', 'traffic', 'frag_index'])\n",
    "\n",
    "design_mat = nyc_geodata[design_vars].to_numpy()\n",
    "design_mat[:, 1] = np.log(design_mat[:, 1])\n",
    "design_mat[:, 2] = np.log(design_mat[:, 2])\n",
    "\n",
    "pois_data = {\"N\":nyc_geodata.shape[0],\n",
    "             \"y\":nyc_geodata['count'].astype('int'),\n",
    "             \"E\":nyc_geodata['kid_pop'].astype('int'),\n",
    "             \"K\":4,\n",
    "             \"xs\":design_mat }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model:  `poisson.stan`\n",
    "\n",
    "This file is in directory `stan/poisson.stan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_model_file = os.path.join('stan', 'poisson.stan')\n",
    "\n",
    "with open(poisson_model_file, 'r') as file:\n",
    "    contents = file.read()\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model is compiled (as needed) on instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_mod = CmdStanModel(stan_file=poisson_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the NUTS-HMC sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_fit = pois_mod.sample(data=pois_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_fit_summary = pois_fit.summary()\n",
    "pois_fit_summary.round(2).loc[\n",
    "  ['beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refinement: Mean-Center Predictor Data\n",
    "\n",
    "*In theory*, much discussion and debate about doing this -\n",
    "start\n",
    "[here](https://www.goldsteinepi.com/blog/thewhyandwhenofcenteringcontinuouspredictorsinregressionmodeling/index.html),\n",
    "follow the links and keep going.\n",
    "\n",
    "*In practice*, ***this helps, alot!*** and is often necessary in order to fit the model.\n",
    "The BRMS package centers continuous data values on zero - [discussion here](https://discourse.mc-stan.org/t/brms-input-scaling-clarification/23601/3).\n",
    "\n",
    "Doing this requires two additions to the model\n",
    "\n",
    "1. In the `transformed data` block, compute the mean of a data column, then subtract the mean from the column.\n",
    "\n",
    "2. If the regression has an intercept term, in the `generated quantities` block, adjust for this by adding back the dot-product of the mean values of each column and the regression coefficient vector to it.\n",
    "\n",
    "There are the key changes / additions to the base model \n",
    "\n",
    "```stan\n",
    "data {\n",
    "  // no change \n",
    "}\n",
    "transformed data {\n",
    "  // center continuous predictors \n",
    "  vector[K] means_xs;  // column means of xs before centering\n",
    "  matrix[N, K] xs_centered;  // centered version of xs\n",
    "  for (k in 1:K) {\n",
    "    means_xs[k] = mean(xs[, k]);\n",
    "    xs_centered[, k] = xs[, k] - means_xs[k];\n",
    "  }\n",
    "}\n",
    "parameters {\n",
    "  // no change \n",
    "}\n",
    "model {\n",
    "  y ~ poisson_log(log_E + beta0 + xs_centered * betas);   // centered data\n",
    "  // priors same\n",
    "}\n",
    "generated quantities {\n",
    "  real beta_intercept = beta0 - dot_product(means_xs, betas);  // adjust intercept\n",
    "  // compute log_lik, y_rep \n",
    "  {\n",
    "    vector[N] eta = log_E + beta0 + xs_centered * betas;   // centered data\n",
    "    //  ..\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_xc_mod = CmdStanModel(stan_file=os.path.join(\n",
    "  'stan', 'poisson_ctr_preds.stan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_xc_fit = pois_xc_mod.sample(data=pois_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_xc_summary = pois_xc_fit.summary()\n",
    "pois_xc_summary.round(2).loc[\n",
    "  ['beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the samples differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data matrix original scales\")   \n",
    "pois_fit_summary.round(2).loc[\n",
    "  ['beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor Data Scales\n",
    "\n",
    "The data variables are on different scales\n",
    "\n",
    "| Measures                                              | Median | Min    | Mean     | Max       |\n",
    "|-------------------------------------------------------|--------|--------|----------|-----------|\n",
    "| Med. household income in USD, 2010-14                 | \\$53,890| \\$9,327 | \\$58,497  | \\$232,266  |\n",
    "| Pct. commute by walk/cycle/public trans, 2010-14      | 73.9   | 9.7    | 69.8     | 100.0     |\n",
    "| Standardized social fragmentation index               | -0.1   | -6.7   | 0.0      | 18.7      |\n",
    "| Traffic Volume (AADT), 2015                           | 19,178 | 843    | 37,248   | 276,476   |\n",
    "\n",
    "In the previous steps, the predictor variables `med_hh_inc` and `traffic` were log-transformed.\n",
    "What happens if we just use the raw data values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_vars = np.array(['pct_pubtransit','med_hh_inc', 'traffic', 'frag_index'])\n",
    "design_mat_2 = nyc_geodata[design_vars].to_numpy()\n",
    "\n",
    "pois_data_2 = {\"N\":nyc_geodata.shape[0],\n",
    "             \"y\":nyc_geodata['count'].astype('int'),\n",
    "             \"E\":nyc_geodata['kid_pop'].astype('int'),\n",
    "             \"K\":4,\n",
    "             \"xs\":design_mat_2 }\n",
    "\n",
    "print(\"log income, traffic\\n\", pd.DataFrame(design_mat).describe())\n",
    "print(\"raw data\\n\", pd.DataFrame(design_mat_2).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the base model on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_fit_2 = pois_mod.sample(data=pois_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_summary_2 = pois_fit_2.summary()\n",
    "pois_summary_2.round(2).loc[\n",
    "  ['beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that transforms the data matrix fails for the same fundamental reason:\n",
    "the regression component `xs * betas` or `xs_centered * betas` cannot be computed,\n",
    "it either overflows or underflow because the predictors are on wildly different scales.\n",
    "(Transforming the data fails faster because it both both `traffic` and `med_hh_inc`\n",
    "include large negative values - this underflows consistently.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment next line - sampler will fail\n",
    "# pois_xc_mod.sample(data=pois_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaway\n",
    "\n",
    "To improve model fit and processing speed, predictors should be on the same scale.\n",
    "\n",
    "*But* remember to account for any rescaling when trying to interpret the fitted coefficients! \n",
    "\n",
    "## Model Checking:  The Posterior Predictive Check \n",
    "\n",
    "From the [Stan Users Guide](https://mc-stan.org/docs/stan-users-guide/posterior-predictive-checks.html#simulating-from-the-posterior-predictive-distribution)\n",
    "\n",
    ">Posterior predictive checks are a way of measuring whether a model\n",
    "does a good job of capturing relevant aspects of the data, such as\n",
    "means, standard deviations, and quantiles.\n",
    "\n",
    ">The posterior predictive distribution is the distribution over new\n",
    "observations given previous observations.  It's predictive in the\n",
    "sense that it's predicting behavior on new data that is not part of\n",
    "the training set.  It's posterior in that everything is conditioned on\n",
    "observed data $y$.\n",
    "\n",
    ">The posterior predictive distribution for replications\n",
    "$y^{\\textrm{rep}}$ of the original data set $y$ given model parameters\n",
    "$\\theta$ is defined by\n",
    "$$\n",
    "p(y^{\\textrm{rep}} \\mid y)\n",
    "= \\int p(y^{\\textrm{rep}} \\mid \\theta)\n",
    "       \\cdot p(\\theta \\mid y) \\, \\textrm{d}\\theta.\n",
    "$$\n",
    "\n",
    "\n",
    "#### Posterior predictive density overlay plots\n",
    "\n",
    "The variable `y_rep` is a *simulated* dataset based on the observed data and the estimated parameters, using Stan's PRNG function `poisson_log_rng`,\n",
    "thus the sample contains a set of random datasets all conditioned on the observed data (nyc_geodata['count']).\n",
    "\n",
    "The posterior predictive density overlay plot compares the distribution of the observed data to the distribution of some of the simulated datasets (`y_rep`).\n",
    "\n",
    "In R, this is available from the [bayesplot](https://mc-stan.org/bayesplot) package as function [`ppc_dens_overlay`](https://mc-stan.org/bayesplot/articles/graphical-ppcs.html#ppc_dens_overlay).\n",
    "\n",
    "To create the equivalent plot in Python:\n",
    "\n",
    "1. select a random set of draws from the output.\n",
    "2. create a ggplot plot\n",
    "3. add  to the plot the density each simulated dataset  (thin line, light color, relatively transparent)\n",
    "4. add to the plot the density of the observed data (heavy line, dark color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rep = pois_xc_fit.stan_variable('y_rep')\n",
    "\n",
    "# select random sample\n",
    "y_rep_sample = pd.DataFrame(y_rep).sample(n=50).reset_index(drop=True).T\n",
    "\n",
    "# create ggplot\n",
    "pois_ppc = p9.ggplot() + p9.ggtitle(\"Posterior Predictive Check: Poisson model\") \n",
    "\n",
    "# add simulated dataset densities\n",
    "for i in range(50):\n",
    "    pois_ppc = (\n",
    "        pois_ppc\n",
    "        + p9.stat_density(mapping=p9.aes(x=y_rep_sample[i]),\n",
    "                          geom='line', color='lightblue', alpha=0.4)\n",
    "    )\n",
    "\n",
    "# add observed data density\n",
    "pois_ppc = (\n",
    "    pois_ppc\n",
    "    + p9.stat_density(data=nyc_geodata, mapping=p9.aes(x='count'),\n",
    "                      geom='line', color='darkblue', size=1.1)\n",
    "    + p9.theme(figure_size=(6,4))\n",
    ")\n",
    "pois_ppc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refinement:  Add random effects\n",
    "\n",
    "The Poisson distribution provides a single parameter $\\lambda$, which is both mean and variance.\n",
    "As the above plots show, the data is overdispersed - the observed variance is greater than expected.\n",
    "To improve the model fit, we can add an ordinary random-effects component - this will account for per-tract heterogeneity.\n",
    "(Not to get head of ourselves, but this is one component in the BYM model).\n",
    "\n",
    "There are the key changes / additions to the base model \n",
    " \n",
    "```stan\n",
    "data {\n",
    "  // no change \n",
    "}\n",
    "transformed data {\n",
    "  // no change, (center continuous predictors)\n",
    "}\n",
    "parameters {\n",
    "  real beta0; // intercept\n",
    "  vector[K] betas; // covariates\n",
    "  vector[N] theta; // heterogeneous random effects\n",
    "  real<lower=0> sigma; // random effects variance \n",
    "}\n",
    "model {\n",
    "  y ~ poisson_log(log_E + beta0 + xs_centered * betas + theta * sigma);\n",
    "  beta0 ~ std_normal();\n",
    "  betas ~ std_normal();\n",
    "  theta ~ std_normal();\n",
    "  sigma ~ normal(0, 5);\n",
    "}\n",
    "generated quantities {\n",
    "  // compute log_lik, y_rep \n",
    "  {\n",
    "    vector[N] eta = log_E + beta0 + xs_centered * betas + theta * sigma;\n",
    "    // ..\n",
    "  }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "**Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_re_mod = CmdStanModel(stan_file=os.path.join(\n",
    "  'stan', 'poisson_re.stan'))\n",
    "pois_re_fit = pois_re_mod.sample(data=pois_data)\n",
    "pois_re_summary = pois_re_fit.summary()\n",
    "pois_re_summary.round(2).loc[\n",
    "  ['beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]', 'sigma']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the samples differ from the base model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_xc_summary.round(2).loc[\n",
    "  ['beta_intercept', 'beta0', 'betas[1]', 'betas[2]', 'betas[3]', 'betas[4]']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the PPC plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rep_re = pois_re_fit.stan_variable('y_rep')\n",
    "\n",
    "# select random sample\n",
    "y_rep_re_sample = pd.DataFrame(y_rep_re).sample(n=50).reset_index(drop=True).T\n",
    "\n",
    "# create ggplot\n",
    "pois_re_ppc = p9.ggplot() + p9.ggtitle(\"Posterior Predictive Check: Poisson + RE model\") \n",
    "\n",
    "# add simulated dataset densities\n",
    "for i in range(50):\n",
    "    pois_re_ppc = (\n",
    "        pois_re_ppc\n",
    "        + p9.stat_density(mapping=p9.aes(x=y_rep_re_sample[i]),\n",
    "                          geom='line', color='lightblue', alpha=0.4)\n",
    "    )\n",
    "\n",
    "# add observed data density\n",
    "pois_re_ppc = (\n",
    "    pois_re_ppc\n",
    "    + p9.stat_density(data=nyc_geodata, mapping=p9.aes(x='count'),\n",
    "                      geom='line', color='darkblue', size=1.1)\n",
    "    + p9.theme(figure_size=(6,4))\n",
    ")\n",
    "pois_re_ppc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_ppc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "\n",
    "#### Leave-one-out cross-validation (LOO)\n",
    "\n",
    "\n",
    "The [loo package](https://mc-stan.org/loo/) provides an implementation of the algorithm presented in\n",
    "\n",
    "* Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 27, 1413â€“1432 (2017). https://doi.org/10.1007/s11222-016-9696-4\n",
    "* Vehtari, A.,Simpson, D.P.,  Gelman, A., Yao, Y., & Gabry, J. Pareto Smoothed Importance Sampling, arxiv, 2024. https://arxiv.org/abs/1507.02646\n",
    "\n",
    "Pareto-smoothed importance sampling (PSIS) provides stabilized effective sample size estimates, Monte Carlo error estimates, and convergence diagnostics and provide a way to efficiently perform\n",
    "leave-one-out (LOO) cross-valiation, which allows us to compare predictive errors between two models.\n",
    "\n",
    "In R,  we can do model comparison using the `loo` package.\n",
    "\n",
    "In Python, we can do this using [ArviZ](https://python.arviz.org/en/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model comparison using loo from Arviz\n",
    "import arviz as az\n",
    "idata_pois_xc = az.from_cmdstanpy(\n",
    "    pois_xc_fit,\n",
    "    posterior_predictive=\"y_rep\",\n",
    "    dims={\"betas\": [\"covariates\"]},\n",
    "    coords={\"covariates\": design_vars},\n",
    "    observed_data={\"y\": pois_data['y']}\n",
    ")\n",
    "idata_pois_xc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ArviZ provides the equivalent functions of those in R packages `bayesplot` and `loo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ppc(idata_pois_xc, data_pairs={\"y\":\"y_rep\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata_pois_re = az.from_cmdstanpy(\n",
    "    pois_re_fit,\n",
    "    posterior_predictive=\"y_rep\",\n",
    "    dims={\"betas\": [\"covariates\"]},\n",
    "    coords={\"covariates\": design_vars},\n",
    "    observed_data={\"y\": pois_data['y']}\n",
    ")\n",
    "az.plot_ppc(idata_pois_re, data_pairs={\"y\":\"y_rep\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.compare({\"poisson\":idata_pois_xc, \"poisson_re\":idata_pois_re})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
